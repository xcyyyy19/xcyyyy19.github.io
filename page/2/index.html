<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> Pravite Home</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/favicon.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      
<link rel="stylesheet" href="/css/fonts/remixicon.css">

      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.staticfile.org/pace/1.2.4/pace.min.js"></script>
       
 

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/npm/@sweetalert2/theme-bulma@5.0.1/bulma.min.css"
      />
      <script src="https://cdn.jsdelivr.net/npm/sweetalert2@11.0.19/dist/sweetalert2.min.js"></script>

      <!-- mermaid -->
      
      <style>
        .swal2-styled.swal2-confirm {
          font-size: 1.6rem;
        }
      </style>
    <link rel="alternate" href="/atom.xml" title="Pravite Home" type="application/atom+xml">
</head>
  </html>
</html>


<body>
  <div id="app">
    
      
      <canvas width="1777" height="841"
        style="position: fixed; left: 0px; top: 0px; z-index: 99999; pointer-events: none;"></canvas>
      
    <main class="content on">
      
<section class="cover">
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">Pravite Home</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
        <img
          src="/images/ayer.svg"
          class="cover-logo"
          alt="Pravite Home"
        />
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.staticfile.org/typed.js/2.0.12/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['群英荟萃，萝卜开会', '圣火昭昭，圣火耀耀，凡我弟子，喵喵喵喵', '不跟唱？您诫过毒吧'],
        startDelay: 30,
        typeSpeed: 100,
        loop: true,
        backSpeed: 50,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  
  
<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content">【网页完善中，1.点击标签-&gt;必看-&gt;网页完善ToDoList。 2.或主页拉到最下方了解进度。】</div>
</div>


<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/周志华《Machine Learning》学习笔记(10)--集成学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(10)--%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2023-06-21T07:42:41.499Z" itemprop="datePublished">2023-06-21</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>上篇主要介绍了鼎鼎大名的EM算法，从算法思想到数学公式推导（边际似然引入隐变量，Jensen不等式简化求导），EM算法实际上可以理解为一种坐标下降法，首先固定一个变量，接着求另外变量的最优解，通过其优美的“两步走”策略能较好地估计隐变量的值。本篇将继续讨论下一类经典算法–集成学习。</p>
<p>#<strong>9、集成学习</strong></p>
<p>顾名思义，集成学习（ensemble learning）指的是将多个学习器进行有效地结合，组建一个“学习器委员会”，其中每个学习器担任委员会成员并行使投票表决权，使得委员会最后的决定更能够四方造福普度众生<del>…</del>，即其泛化性能要能优于其中任何一个学习器。</p>
<p>##<strong>9.1 个体与集成</strong></p>
<p>集成学习的基本结构为：先产生一组个体学习器，再使用某种策略将它们结合在一起。集成模型如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0c15683.png" alt="1.png"></p>
<p>在上图的集成模型中，若个体学习器都属于同一类别，例如都是决策树或都是神经网络，则称该集成为同质的（homogeneous）;若个体学习器包含多种类型的学习算法，例如既有决策树又有神经网络，则称该集成为异质的（heterogenous）。</p>
<blockquote>
<p><strong>同质集成</strong>：个体学习器称为“基学习器”（base learner），对应的学习算法为“基学习算法”（base learning algorithm）。<br><strong>异质集成</strong>：个体学习器称为“组件学习器”（component learner）或直称为“个体学习器”。</p>
</blockquote>
<p>上面我们已经提到要让集成起来的泛化性能比单个学习器都要好，虽说团结力量大但也有木桶短板理论调皮捣蛋，那如何做到呢？这就引出了集成学习的两个重要概念：<strong>准确性</strong>和<strong>多样性</strong>（diversity）。准确性指的是个体学习器不能太差，要有一定的准确度；多样性则是个体学习器之间的输出要具有差异性。通过下面的这三个例子可以很容易看出这一点，准确度较高，差异度也较高，可以较好地提升集成性能。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d23e13.png" alt="2.png"></p>
<p>现在考虑二分类的简单情形，假设基分类器之间相互独立（能提供较高的差异度），且错误率相等为 ε，则可以将集成器的预测看做一个伯努利实验，易知当所有基分类器中不足一半预测正确的情况下，集成器预测错误，所以集成器的错误率可以计算为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0cce0bb.png" alt="3.png"></p>
<p>此时，集成器错误率随着基分类器的个数的增加呈指数下降，但前提是基分类器之间相互独立，在实际情形中显然是不可能的，假设训练有A和B两个分类器，对于某个测试样本，显然满足：P（A&#x3D;1 | B&#x3D;1）&gt; P（A&#x3D;1），因为A和B为了解决相同的问题而训练，因此在预测新样本时存在着很大的联系。因此，<strong>个体学习器的“准确性”和“差异性”本身就是一对矛盾的变量</strong>，准确性高意味着牺牲多样性，所以产生“<strong>好而不同</strong>”的个体学习器正是集成学习研究的核心。现阶段有三种主流的集成学习方法：Boosting、Bagging以及随机森林（Random Forest），接下来将进行逐一介绍。</p>
<p>##<strong>9.2 Boosting</strong></p>
<p>Boosting是一种串行的工作机制，即个体学习器的训练存在依赖关系，必须一步一步序列化进行。其基本思想是：增加前一个基学习器在训练训练过程中预测错误样本的权重，使得后续基学习器更加关注这些打标错误的训练样本，尽可能纠正这些错误，一直向下串行直至产生需要的T个基学习器，Boosting最终对这T个学习器进行加权结合，产生学习器委员会。</p>
<p>Boosting族算法最著名、使用最为广泛的就是AdaBoost，因此下面主要是对AdaBoost算法进行介绍。AdaBoost使用的是<strong>指数损失函数</strong>，因此AdaBoost的权值与样本分布的更新都是围绕着最小化指数损失函数进行的。看到这里回想一下之前的机器学习算法，<strong>不难发现机器学习的大部分带参模型只是改变了最优化目标中的损失函数</strong>：如果是Square loss，那就是最小二乘了；如果是Hinge Loss，那就是著名的SVM了；如果是log-Loss，那就是Logistic Regression了。</p>
<p>定义基学习器的集成为加权结合，则有：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ca2ca5.png" alt="4.png"></p>
<p>AdaBoost算法的指数损失函数定义为：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d10461.png" alt="5.png"></p>
<p>具体说来，整个Adaboost 迭代算法分为3步：</p>
<ul>
<li>初始化训练数据的权值分布。如果有N个样本，则每一个训练样本最开始时都被赋予相同的权值：1&#x2F;N。</li>
<li>训练弱分类器。具体训练过程中，如果某个样本点已经被准确地分类，那么在构造下一个训练集中，它的权值就被降低；相反，如果某个样本点没有被准确地分类，那么它的权值就得到提高。然后，权值更新过的样本集被用于训练下一个分类器，整个训练过程如此迭代地进行下去。</li>
<li>将各个训练得到的弱分类器组合成强分类器。各个弱分类器的训练过程结束后，加大分类误差率小的弱分类器的权重，使其在最终的分类函数中起着较大的决定作用，而降低分类误差率大的弱分类器的权重，使其在最终的分类函数中起着较小的决定作用。</li>
</ul>
<p>整个AdaBoost的算法流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d7c057.png" alt="6.png"></p>
<p>可以看出：<strong>AdaBoost的核心步骤就是计算基学习器权重和样本权重分布</strong>，那为何是上述的计算公式呢？这就涉及到了我们之前为什么说大部分带参机器学习算法只是改变了损失函数，就是因为<strong>大部分模型的参数都是通过最优化损失函数（可能还加个规则项）而计算（梯度下降，坐标下降等）得到</strong>，这里正是通过最优化指数损失函数从而得到这两个参数的计算公式，具体的推导过程此处不进行展开。</p>
<p>Boosting算法要求基学习器能对特定分布的数据进行学习，即每次都更新样本分布权重，这里书上提到了两种方法：“重赋权法”（re-weighting）和“重采样法”（re-sampling），书上的解释有些晦涩，这里进行展开一下：</p>
<blockquote>
<p><strong>重赋权法</strong> : 对每个样本附加一个权重，这时涉及到样本属性与标签的计算，都需要乘上一个权值。<br><strong>重采样法</strong> : 对于一些无法接受带权样本的及学习算法，适合用“重采样法”进行处理。方法大致过程是，根据各个样本的权重，对训练数据进行重采样，初始时样本权重一样，每个样本被采样到的概率一致，每次从N个原始的训练样本中按照权重有放回采样N个样本作为训练集，然后计算训练集错误率，然后调整权重，重复采样，集成多个基学习器。</p>
</blockquote>
<p>从偏差-方差分解来看：Boosting算法主要关注于降低偏差，每轮的迭代都关注于训练过程中预测错误的样本，将弱学习提升为强学习器。从AdaBoost的算法流程来看，标准的AdaBoost只适用于二分类问题。在此，当选为数据挖掘十大算法之一的AdaBoost介绍到这里，能够当选正是说明这个算法十分婀娜多姿，背后的数学证明和推导充分证明了这一点，限于篇幅不再继续展开。</p>
<p>##<strong>9.3 Bagging与Random Forest</strong></p>
<p>相比之下，Bagging与随机森林算法就简洁了许多，上面已经提到产生“好而不同”的个体学习器是集成学习研究的核心，即在保证基学习器准确性的同时增加基学习器之间的多样性。而这两种算法的基本思（tao）想（lu）都是通过“自助采样”的方法来增加多样性。</p>
<p>###<strong>9.3.1 Bagging</strong></p>
<p>Bagging是一种并行式的集成学习方法，即基学习器的训练之间没有前后顺序可以同时进行，Bagging使用“有放回”采样的方式选取训练集，对于包含m个样本的训练集，进行m次有放回的随机采样操作，从而得到m个样本的采样集，这样训练集中有接近36.8%的样本没有被采到。按照相同的方式重复进行，我们就可以采集到T个包含m个样本的数据集，从而训练出T个基学习器，最终对这T个基学习器的输出进行结合。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0ce62fc.png" alt="7.png"></p>
<p>Bagging算法的流程如下所示：</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d0e761.png" alt="8.png"></p>
<p>可以看出Bagging主要通过<strong>样本的扰动</strong>来增加基学习器之间的多样性，因此Bagging的基学习器应为那些对训练集十分敏感的不稳定学习算法，例如：神经网络与决策树等。从偏差-方差分解来看，Bagging算法主要关注于降低方差，即通过多次重复训练提高稳定性。不同于AdaBoost的是，Bagging可以十分简单地移植到多分类、回归等问题。总的说起来则是：<strong>AdaBoost关注于降低偏差，而Bagging关注于降低方差。</strong></p>
<p>###<strong>9.3.2 随机森林</strong></p>
<p>随机森林（Random Forest）是Bagging的一个拓展体，它的基学习器固定为决策树，多棵树也就组成了森林，而“随机”则在于选择划分属性的随机，随机森林在训练基学习器时，也采用有放回采样的方式添加样本扰动，同时它还引入了一种<strong>属性扰动</strong>，即在基决策树的训练过程中，在选择划分属性时，RF先从候选属性集中随机挑选出一个包含K个属性的子集，再从这个子集中选择最优划分属性，一般推荐K&#x3D;log2（d）。</p>
<p>这样随机森林中基学习器的多样性不仅来自样本扰动，还来自属性扰动，从而进一步提升了基学习器之间的差异度。相比决策树的Bagging集成，随机森林的起始性能较差（由于属性扰动，基决策树的准确度有所下降），但随着基学习器数目的增多，随机森林往往会收敛到更低的泛化误差。同时不同于Bagging中决策树从所有属性集中选择最优划分属性，随机森林只在属性集的一个子集中选择划分属性，因此训练效率更高。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d7a4fd.png" alt="9.png"></p>
<p>##<strong>9.4 结合策略</strong></p>
<p>结合策略指的是在训练好基学习器后，如何将这些基学习器的输出结合起来产生集成模型的最终输出，下面将介绍一些常用的结合策略：</p>
<p>###<strong>9.4.1 平均法（回归问题）</strong></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84d0d07983.png" alt="10.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1b74ff.png" alt="11.png"></p>
<p>易知简单平均法是加权平均法的一种特例，加权平均法可以认为是集成学习研究的基本出发点。由于各个基学习器的权值在训练中得出，<strong>一般而言，在个体学习器性能相差较大时宜使用加权平均法，在个体学习器性能相差较小时宜使用简单平均法</strong>。</p>
<p>###<strong>9.4.2 投票法（分类问题）</strong></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de2629c4.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25a74b.png" alt="13.png"></p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de1bacc4.png" alt="14.png"></p>
<p>绝对多数投票法（majority voting）提供了拒绝选项，这在可靠性要求很高的学习任务中是一个很好的机制。同时，对于分类任务，各个基学习器的输出值有两种类型，分别为类标记和类概率。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de2768c1.png" alt="15.png"></p>
<p>一些在产生类别标记的同时也生成置信度的学习器，置信度可转化为类概率使用，<strong>一般基于类概率进行结合往往比基于类标记进行结合的效果更好</strong>，需要注意的是对于异质集成，其类概率不能直接进行比较，此时需要将类概率转化为类标记输出，然后再投票。</p>
<p>###<strong>9.4.3 学习法</strong></p>
<p>学习法是一种更高级的结合策略，即学习出一种“投票”的学习器，Stacking是学习法的典型代表。Stacking的基本思想是：首先训练出T个基学习器，对于一个样本它们会产生T个输出，将这T个基学习器的输出与该样本的真实标记作为新的样本，m个样本就会产生一个m*T的样本集，来训练一个新的“投票”学习器。投票学习器的输入属性与学习算法对Stacking集成的泛化性能有很大的影响，书中已经提到：<strong>投票学习器采用类概率作为输入属性，选用多响应线性回归（MLR）一般会产生较好的效果</strong>。</p>
<p><img src="https://i.loli.net/2018/10/18/5bc84de25cbaf.png" alt="16.png"></p>
<p>##<strong>9.5 多样性（diversity）</strong></p>
<p>在集成学习中，基学习器之间的多样性是影响集成器泛化性能的重要因素。因此增加多样性对于集成学习研究十分重要，一般的思路是在学习过程中引入随机性，常见的做法主要是对数据样本、输入属性、输出表示、算法参数进行扰动。</p>
<blockquote>
<p><strong>数据样本扰动</strong>，即利用具有差异的数据集来训练不同的基学习器。例如：有放回自助采样法，但此类做法只对那些不稳定学习算法十分有效，例如：决策树和神经网络等，训练集的稍微改变能导致学习器的显著变动。<br><strong>输入属性扰动</strong>，即随机选取原空间的一个子空间来训练基学习器。例如：随机森林，从初始属性集中抽取子集，再基于每个子集来训练基学习器。但若训练集只包含少量属性，则不宜使用属性扰动。<br><strong>输出表示扰动</strong>，此类做法可对训练样本的类标稍作变动，或对基学习器的输出进行转化。<br><strong>算法参数扰动</strong>，通过随机设置不同的参数，例如：神经网络中，随机初始化权重与随机设置隐含层节点数。</p>
</blockquote>
<p>在此，集成学习就介绍完毕，看到这里，大家也会发现集成学习实质上是一种通用框架，可以使用任何一种基学习器，从而改进单个学习器的泛化性能。据说数据挖掘竞赛KDDCup历年的冠军几乎都使用了集成学习，看来的确是个好东西~</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/README"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/README/" class="article-date">
  <time datetime="2023-06-21T07:42:41.482Z" itemprop="datePublished">2023-06-21</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>本文为周志华《机器学习》的学习笔记，记录了本人在学习这本书的过程中的理解思路以及一些有助于消化书内容的拓展知识，笔记中参考了许多网上的大牛经典博客以及李航《统计学习》的内容，向前辈们和知识致敬！</p>
<blockquote>
<p>啃西瓜学习交流QQ群: 531701485， 遇到疑问可以随时在上面提问，并会不定期分享优质的机器&#x2F;深度学习资料，欢迎各位机器学习爱好者的入群讨论学习!</p>
</blockquote>
<blockquote>
<p>作者目前在腾讯微信事业群从事大数据、机器学习、推荐系统工作，团队会定期输出：机器学习在实际业务领域的技术总结，欢迎大家关注学习，共同进步！</p>
</blockquote>
<img src="https://ftp.bmp.ovh/imgs/2021/07/022f98f41a401e3a.jpeg" width="250px" height="250px" />

 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/周志华《Machine Learning》学习笔记(1)--绪论"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)--%E7%BB%AA%E8%AE%BA/"
    >西瓜书</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(1)--%E7%BB%AA%E8%AE%BA/" class="article-date">
  <time datetime="2023-06-21T03:43:25.000Z" itemprop="datePublished">2023-06-21</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>机器学习是目前信息技术中最激动人心的方向之一，其应用已经深入到生活的各个层面且与普通人的日常生活密切相关。本文为清华大学最新出版的《机器学习》教材的Learning Notes，书作者是南京大学周志华教授，多个大陆首位彰显其学术奢华。本篇主要介绍了该教材前两个章节的知识点以及自己一点浅陋的理解。</p>
<p><strong>1  绪论</strong></p>
<p>傍晚小街路面上沁出微雨后的湿润，和熙的细风吹来，抬头看看天边的晚霞，嗯，明天又是一个好天气。走到水果摊旁，挑了个根蒂蜷缩、敲起来声音浊响的青绿西瓜，一边满心期待着皮薄肉厚瓢甜的爽落感，一边愉快地想着，这学期狠下了工夫，基础概念弄得清清楚楚，算法作业也是信手拈来，这门课成绩一定差不了！哈哈，也希望自己这学期的machine learning课程取得一个好成绩！</p>
<p><strong>1.1 机器学习的定义</strong></p>
<p>正如我们根据过去的经验来判断明天的天气，吃货们希望从购买经验中挑选一个好瓜，那能不能让计算机帮助人类来实现这个呢？机器学习正是这样的一门学科，人的“经验”对应计算机中的“数据”，让计算机来学习这些经验数据，生成一个算法模型，在面对新的情况中，计算机便能作出有效的判断，这便是机器学习。</p>
<p>另一本经典教材的作者Mitchell给出了一个形式化的定义，假设：</p>
<ul>
<li>P：计算机程序在某任务类T上的性能。</li>
<li>T：计算机程序希望实现的任务类。</li>
<li>E：表示经验，即历史的数据集。</li>
</ul>
<p>若该计算机程序通过利用经验E在任务T上获得了性能P的改善，则称该程序对E进行了学习。</p>
<p><strong>1.2 机器学习的一些基本术语</strong></p>
<p>假设我们收集了一批西瓜的数据，例如：（色泽&#x3D;青绿;根蒂&#x3D;蜷缩;敲声&#x3D;浊响)， (色泽&#x3D;乌黑;根蒂&#x3D;稍蜷;敲声&#x3D;沉闷)， (色泽&#x3D;浅自;根蒂&#x3D;硬挺;敲声&#x3D;清脆)……每对括号内是一个西瓜的记录，定义：	 </p>
<ul>
<li><p>所有记录的集合为：数据集。</p>
</li>
<li><p>每一条记录为：一个实例（instance）或样本（sample）。</p>
</li>
<li><p>例如：色泽或敲声，单个的特点为特征（feature）或属性（attribute）。</p>
</li>
<li><p>对于一条记录，如果在坐标轴上表示，每个西瓜都可以用坐标轴中的一个点表示，一个点也是一个向量，例如（青绿，蜷缩，浊响），即每个西瓜为：一个特征向量（feature vector）。</p>
</li>
<li><p>一个样本的特征数为：维数（dimensionality），该西瓜的例子维数为3，当维数非常大时，也就是现在说的“维数灾难”。</p>
<p> 计算机程序学习经验数据生成算法模型的过程中，每一条记录称为一个“训练样本”，同时在训练好模型后，我们希望使用新的样本来测试模型的效果，则每一个新的样本称为一个“测试样本”。定义：	</p>
</li>
<li><p>所有训练样本的集合为：训练集（trainning set），[特殊]。</p>
</li>
<li><p>所有测试样本的集合为：测试集（test set），[一般]。  </p>
</li>
<li><p>机器学习出来的模型适用于新样本的能力为：泛化能力（generalization），即从特殊到一般。</p>
<p> 西瓜的例子中，我们是想计算机通过学习西瓜的特征数据，训练出一个决策模型，来判断一个新的西瓜是否是好瓜。可以得知我们预测的是：西瓜是好是坏，即好瓜与差瓜两种，是离散值。同样地，也有通过历年的人口数据，来预测未来的人口数量，人口数量则是连续值。定义：	</p>
</li>
<li><p>预测值为离散值的问题为：分类（classification）。</p>
</li>
<li><p>预测值为连续值的问题为：回归（regression）。</p>
<p> 我们预测西瓜是否是好瓜的过程中，很明显对于训练集中的西瓜，我们事先已经知道了该瓜是否是好瓜，学习器通过学习这些好瓜或差瓜的特征，从而总结出规律，即训练集中的西瓜我们都做了标记，称为标记信息。但也有没有标记信息的情形，例如：我们想将一堆西瓜根据特征分成两个小堆，使得某一堆的西瓜尽可能相似，即都是好瓜或差瓜，对于这种问题，我们事先并不知道西瓜的好坏，样本没有标记信息。定义：	</p>
</li>
<li><p>训练数据有标记信息的学习任务为：监督学习（supervised learning），容易知道上面所描述的分类和回归都是监督学习的范畴。</p>
</li>
<li><p>训练数据没有标记信息的学习任务为：无监督学习（unsupervised learning），常见的有聚类和关联规则。</p>
</li>
</ul>
<p><strong>2  模型的评估与选择</strong></p>
<p><strong>2.1 误差与过拟合</strong></p>
<p>我们将学习器对样本的实际预测结果与样本的真实值之间的差异成为：误差（error）。定义：	</p>
<ul>
<li>在训练集上的误差称为训练误差（training error）或经验误差（empirical error）。</li>
<li>在测试集上的误差称为测试误差（test error）。</li>
<li>学习器在所有新样本上的误差称为泛化误差（generalization error）。</li>
</ul>
<p>显然，我们希望得到的是在新样本上表现得很好的学习器，即泛化误差小的学习器。因此，我们应该让学习器尽可能地从训练集中学出普适性的“一般特征”，这样在遇到新样本时才能做出正确的判别。然而，当学习器把训练集学得“太好”的时候，即把一些训练样本的自身特点当做了普遍特征；同时也有学习能力不足的情况，即训练集的基本特征都没有学习出来。我们定义：</p>
<ul>
<li>学习能力过强，以至于把训练样本所包含的不太一般的特性都学到了，称为：过拟合（overfitting）。</li>
<li>学习能太差，训练样本的一般性质尚未学好，称为：欠拟合（underfitting）。</li>
</ul>
<p>可以得知：在过拟合问题中，训练误差十分小，但测试误差教大；在欠拟合问题中，训练误差和测试误差都比较大。目前，欠拟合问题比较容易克服，例如增加迭代次数等，但过拟合问题还没有十分好的解决方案，过拟合是机器学习面临的关键障碍。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7181172996.png"></p>
<p><strong>2.2 评估方法</strong></p>
<p>在现实任务中，我们往往有多种算法可供选择，那么我们应该选择哪一个算法才是最适合的呢？如上所述，我们希望得到的是泛化误差小的学习器，理想的解决方案是对模型的泛化误差进行评估，然后选择泛化误差最小的那个学习器。但是，泛化误差指的是模型在所有新样本上的适用能力，我们无法直接获得泛化误差。</p>
<p>因此，通常我们采用一个“测试集”来测试学习器对新样本的判别能力，然后以“测试集”上的“测试误差”作为“泛化误差”的近似。显然：我们选取的测试集应尽可能与训练集互斥，下面用一个小故事来解释why：</p>
<p>假设老师出了10 道习题供同学们练习，考试时老师又用同样的这10道题作为试题，可能有的童鞋只会做这10 道题却能得高分，很明显：这个考试成绩并不能有效地反映出真实水平。回到我们的问题上来，我们希望得到泛化性能好的模型，好比希望同学们课程学得好并获得了对所学知识”举一反三”的能力；训练样本相当于给同学们练习的习题，测试过程则相当于考试。显然，若测试样本被用作训练了，则得到的将是过于”乐观”的估计结果。</p>
<p><strong>2.3 训练集与测试集的划分方法</strong></p>
<p>如上所述：我们希望用一个“测试集”的“测试误差”来作为“泛化误差”的近似，因此我们需要对初始数据集进行有效划分，划分出互斥的“训练集”和“测试集”。下面介绍几种常用的划分方法：</p>
<p><strong>2.3.1 留出法</strong></p>
<p>将数据集D划分为两个互斥的集合，一个作为训练集S，一个作为测试集T，满足D&#x3D;S∪T且S∩T&#x3D;∅，常见的划分为：大约2&#x2F;3-4&#x2F;5的样本用作训练，剩下的用作测试。需要注意的是：训练&#x2F;测试集的划分要尽可能保持数据分布的一致性，以避免由于分布的差异引入额外的偏差，常见的做法是采取分层抽样。同时，由于划分的随机性，单次的留出法结果往往不够稳定，一般要采用若干次随机划分，重复实验取平均值的做法。</p>
<p><strong>2.3.2 交叉验证法</strong></p>
<p>将数据集D划分为k个大小相同的互斥子集，满足D&#x3D;D1∪D2∪…∪Dk，Di∩Dj&#x3D;∅（i≠j），同样地尽可能保持数据分布的一致性，即采用分层抽样的方法获得这些子集。交叉验证法的思想是：每次用k-1个子集的并集作为训练集，余下的那个子集作为测试集，这样就有K种训练集&#x2F;测试集划分的情况，从而可进行k次训练和测试，最终返回k次测试结果的均值。交叉验证法也称“k折交叉验证”，k最常用的取值是10，下图给出了10折交叉验证的示意图。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc718115d224.png"></p>
<p>与留出法类似，将数据集D划分为K个子集的过程具有随机性，因此K折交叉验证通常也要重复p次，称为p次k折交叉验证，常见的是10次10折交叉验证，即进行了100次训练&#x2F;测试。特殊地当划分的k个子集的每个子集中只有一个样本时，称为“留一法”，显然，留一法的评估结果比较准确，但对计算机的消耗也是巨大的。</p>
<p><strong>2.3.3 自助法</strong></p>
<p>我们希望评估的是用整个D训练出的模型。但在留出法和交叉验证法中，由于保留了一部分样本用于测试，因此实际评估的模型所使用的训练集比D小，这必然会引入一些因训练样本规模不同而导致的估计偏差。留一法受训练样本规模变化的影响较小，但计算复杂度又太高了。“自助法”正是解决了这样的问题。</p>
<p>自助法的基本思想是：给定包含m个样本的数据集D，每次随机从D 中挑选一个样本，将其拷贝放入D’，然后再将该样本放回初始数据集D 中，使得该样本在下次采样时仍有可能被采到。重复执行m 次，就可以得到了包含m个样本的数据集D’。可以得知在m次采样中，样本始终不被采到的概率取极限为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71811246dd.png"></p>
<p>这样，通过自助采样，初始样本集D中大约有36.8%的样本没有出现在D’中，于是可以将D’作为训练集，D-D’作为测试集。自助法在数据集较小，难以有效划分训练集&#x2F;测试集时很有用，但由于自助法产生的数据集（随机抽样）改变了初始数据集的分布，因此引入了估计偏差。在初始数据集足够时，留出法和交叉验证法更加常用。</p>
<p><strong>2.4 调参</strong></p>
<p>大多数学习算法都有些参数(parameter) 需要设定，参数配置不同，学得模型的性能往往有显著差别，这就是通常所说的”参数调节”或简称”调参” (parameter tuning)。</p>
<p>学习算法的很多参数是在实数范围内取值，因此，对每种参数取值都训练出模型来是不可行的。常用的做法是：对每个参数选定一个范围和步长λ，这样使得学习的过程变得可行。例如：假定算法有3 个参数，每个参数仅考虑5 个候选值，这样对每一组训练&#x2F;测试集就有5<em>5</em>5&#x3D; 125 个模型需考察，由此可见：拿下一个参数（即经验值）对于算法人员来说是有多么的happy。</p>
<p>最后需要注意的是：当选定好模型和调参完成后，我们需要使用初始的数据集D重新训练模型，即让最初划分出来用于评估的测试集也被模型学习，增强模型的学习效果。用上面考试的例子来比喻：就像高中时大家每次考试完，要将考卷的题目消化掉（大多数题目都还是之前没有见过的吧？），这样即使考差了也能开心的玩耍了~。</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/周志华《Machine Learning》学习笔记(2)--性能度量"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/"
    >西瓜书</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(2)--%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F/" class="article-date">
  <time datetime="2023-06-21T03:43:25.000Z" itemprop="datePublished">2023-06-21</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>本篇主要是对第二章剩余知识的理解，包括：性能度量、比较检验和偏差与方差。在上一篇中，我们解决了评估学习器泛化性能的方法，即用测试集的“测试误差”作为“泛化误差”的近似，当我们划分好训练&#x2F;测试集后，那如何计算“测试误差”呢？这就是性能度量，例如：均方差，错误率等，即“测试误差”的一个评价标准。有了评估方法和性能度量，就可以计算出学习器的“测试误差”，但由于“测试误差”受到很多因素的影响，例如：算法随机性或测试集本身的选择，那如何对两个或多个学习器的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。写到后面发现冗长之后读起来十分没有快感，故本篇主要知识点为性能度量。</p>
<p><strong>2.5 性能度量</strong></p>
<p>性能度量（performance measure）是衡量模型泛化能力的评价标准，在对比不同模型的能力时，使用不同的性能度量往往会导致不同的评判结果。本节除2.5.1外，其它主要介绍分类模型的性能度量。</p>
<p><strong>2.5.1 最常见的性能度量</strong></p>
<p>在回归任务中，即预测连续值的问题，最常用的性能度量是“均方误差”（mean squared error）,很多的经典算法都是采用了MSE作为评价函数，想必大家都十分熟悉。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf76276.png" alt="1.png"></p>
<p>在分类任务中，即预测离散值的问题，最常用的是错误率和精度，错误率是分类错误的样本数占样本总数的比例，精度则是分类正确的样本数占样本总数的比例，易知：错误率+精度&#x3D;1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf4c704.png" alt="2.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf6fb84.png" alt="3.png"></p>
<p><strong>2.5.2 查准率&#x2F;查全率&#x2F;F1</strong></p>
<p>错误率和精度虽然常用，但不能满足所有的需求，例如：在推荐系统中，我们只关心推送给用户的内容用户是否感兴趣（即查准率），或者说所有用户感兴趣的内容我们推送出来了多少（即查全率）。因此，使用查准&#x2F;查全率更适合描述这类问题。对于二分类问题，分类结果混淆矩阵与查准&#x2F;查全率定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf885a4.png" alt="4.png"></p>
<p>初次接触时，FN与FP很难正确的理解，按照惯性思维容易把FN理解成：False-&gt;Negtive，即将错的预测为错的，这样FN和TN就反了，后来找到一张图，描述得很详细，为方便理解，把这张图也贴在了下边：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf871a6.png" alt="5.png"></p>
<p>正如天下没有免费的午餐，查准率和查全率是一对矛盾的度量。例如我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，查全率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样查准率就很低了。</p>
<p>“P-R曲线”正是描述查准&#x2F;查全率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71dafc4411.png" alt="6.png"></p>
<p>P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P&#x3D;R时的取值，平衡点的取值越高，性能更优。</p>
<p>P和R指标有时会出现矛盾的情况，这样就需要综合考虑他们，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf40ff6.png" alt="7.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf75407.png" alt="8.png"></p>
<p>特别地，当β&#x3D;1时，也就是常见的F1度量，是P和R的调和平均，当F1较高时，模型的性能越好。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf20885.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71daf4b90a.png" alt="10.png"></p>
<p>有时候我们会有多个二分类混淆矩阵，例如：多次训练或者在多个数据集上训练，那么估算全局性能的方法有两种，分为宏观和微观。简单理解，宏观就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，在算出Fβ或F1，而微观则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出Fβ或F1。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed70230e.png" alt="11.png"></p>
<p><strong>2.5.3 ROC与AUC</strong></p>
<p>如上所述：学习器对测试样本的评估结果一般为一个实值或概率，设定一个阈值，大于阈值为正例，小于阈值为负例，因此这个实值的好坏直接决定了学习器的泛化性能，若将这些实值排序，则排序的好坏决定了学习器的性能高低。ROC曲线正是从这个角度出发来研究学习器的泛化性能，ROC曲线与P-R曲线十分类似，都是按照排序的顺序逐一按照正例预测，不同的是ROC曲线以“真正例率”（True Positive Rate，简称TPR）为横轴，纵轴为“假正例率”（False Positive Rate，简称FPR），ROC偏重研究基于测试样本评估值的排序好坏。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6bee91.png" alt="12.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed75cefe.png" alt="13.png"></p>
<p>简单分析图像，可以得知：当FN&#x3D;0时，TN也必须0，反之也成立，我们可以画一个队列，试着使用不同的截断点（即阈值）去分割队列，来分析曲线的形状，（0,0）表示将所有的样本预测为负例，（1,1）则表示将所有的样本预测为正例，（0,1）表示正例全部出现在负例之前的理想情况，（1,0）则表示负例全部出现在正例之前的最差情况。限于篇幅，这里不再论述。</p>
<p>现实中的任务通常都是有限个测试样本，因此只能绘制出近似ROC曲线。绘制方法：首先根据测试样本的评估值对测试样本排序，接着按照以下规则进行绘制。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed740a24.png" alt="14.png"></p>
<p>同样地，进行模型的性能比较时，若一个学习器A的ROC曲线被另一个学习器B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。ROC曲线下的面积定义为AUC（Area Uder ROC Curve），不同于P-R的是，这里的AUC是可估算的，即AOC曲线下每一个小矩形的面积之和。易知：AUC越大，证明排序的质量越好，AUC为1时，证明所有正例排在了负例的前面，AUC为0时，所有的负例排在了正例的前面。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6e2c57.png" alt="15.png"></p>
<p><strong>2.5.4 代价敏感错误率与代价曲线</strong></p>
<p>上面的方法中，将学习器的犯错同等对待，但在现实生活中，将正例预测成假例与将假例预测成正例的代价常常是不一样的，例如：将无疾病–&gt;有疾病只是增多了检查，但有疾病–&gt;无疾病却是增加了生命危险。以二分类为例，由此引入了“代价矩阵”（cost matrix）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6ed582.png" alt="16.png"></p>
<p>在非均等错误代价下，我们希望的是最小化“总体代价”，这样“代价敏感”的错误率（2.5.1节介绍）为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed70bebe.png" alt="17.png"></p>
<p>同样对于ROC曲线，在非均等错误代价下，演变成了“代价曲线”，代价曲线横轴是取值在[0,1]之间的正例概率代价，式中p表示正例的概率，纵轴是取值为[0,1]的归一化代价。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6e952e.png" alt="18.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed6eee7b.png" alt="19.png"></p>
<p>代价曲线的绘制很简单：设ROC曲线上一点的坐标为(TPR，FPR) ，则可相应计算出FNR，然后在代价平面上绘制一条从(0，FPR) 到(1，FNR) 的线段，线段下的面积即表示了该条件下的期望总体代价；如此将ROC 曲线土的每个点转化为代价平面上的一条线段，然后取所有线段的下界，围成的面积即为在所有条件下学习器的期望总体代价，如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc71ed716e0d.png" alt="20.png"></p>
<p>在此模型的性能度量方法就介绍完了，以前一直以为均方误差和精准度就可以了，现在才发现天空如此广阔~</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/周志华《Machine Learning》学习笔记(3)--假设检验&amp;方差&amp;偏差"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(3)--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&%E6%96%B9%E5%B7%AE&%E5%81%8F%E5%B7%AE/"
    >西瓜书</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(3)--%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&%E6%96%B9%E5%B7%AE&%E5%81%8F%E5%B7%AE/" class="article-date">
  <time datetime="2023-06-21T03:43:25.000Z" itemprop="datePublished">2023-06-21</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>在上两篇中，我们介绍了多种常见的评估方法和性能度量标准，这样我们就可以根据数据集以及模型任务的特征，选择出最合适的评估和性能度量方法来计算出学习器的“测试误差“。但由于“测试误差”受到很多因素的影响，例如：算法随机性(例如常见的K-Means)或测试集本身的选择，使得同一模型每次得到的结果不尽相同，同时测试误差是作为泛化误差的近似，并不能代表学习器真实的泛化性能，那如何对单个或多个学习器在不同或相同测试集上的性能度量结果做比较呢？这就是比较检验。最后偏差与方差是解释学习器泛化性能的一种重要工具。本篇延续上一篇的内容，主要讨论了比较检验、方差与偏差。</p>
<p>##<strong>2.6 比较检验</strong></p>
<p>在比较学习器泛化性能的过程中，统计假设检验（hypothesis test）为学习器性能比较提供了重要依据，即若A在某测试集上的性能优于B，那A学习器比B好的把握有多大。 为方便论述，本篇中都是以“错误率”作为性能度量的标准。</p>
<p>###<strong>2.6.1 假设检验</strong></p>
<p>“假设”指的是对样本总体的分布或已知分布中某个参数值的一种猜想，例如：假设总体服从泊松分布，或假设正态总体的期望u&#x3D;u0。回到本篇中，我们可以通过测试获得测试错误率，但直观上测试错误率和泛化错误率相差不会太远，因此可以通过测试错误率来推测泛化错误率的分布，这就是一种假设检验。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211aed8e3.png" alt="1.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a5817d.png" alt="2.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a336b5.png" alt="3.png"></p>
<p>###<strong>2.6.2 交叉验证t检验</strong></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a68ef9.png" alt="4.png"></p>
<p>###<strong>2.6.3 McNemar检验</strong></p>
<p>MaNemar主要用于二分类问题，与成对t检验一样也是用于比较两个学习器的性能大小。主要思想是：若两学习器的性能相同，则A预测正确B预测错误数应等于B预测错误A预测正确数，即e01&#x3D;e10，且|e01-e10|服从N（1，e01+e10）分布。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a2c7f9.png" alt="5.png"></p>
<p>因此，如下所示的变量服从自由度为1的卡方分布，即服从标准正态分布N（0,1）的随机变量的平方和，下式只有一个变量，故自由度为1，检验的方法同上：做出假设–&gt;求出满足显著度的临界点–&gt;给出拒绝域–&gt;验证假设。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a34e96.png" alt="6.png"></p>
<p>###<strong>2.6.4 Friedman检验与Nemenyi后续检验</strong></p>
<p>上述的三种检验都只能在一组数据集上，F检验则可以在多组数据集进行多个学习器性能的比较，基本思想是在同一组数据集上，根据测试结果（例：测试错误率）对学习器的性能进行排序，赋予序值1,2,3…，相同则平分序值，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a2db45.png" alt="7.png"></p>
<p>若学习器的性能相同，则它们的平均序值应该相同，且第i个算法的平均序值ri服从正态分布N（（k+1）&#x2F;2，（k+1）(k-1)&#x2F;12），则有：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a45349.png" alt="8.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a2684c.png" alt="9.png"></p>
<p>服从自由度为k-1和(k-1)(N-1)的F分布。下面是F检验常用的临界值：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc7211a7e3f0.png" alt="10.png"></p>
<p>若“H0：所有算法的性能相同”这个假设被拒绝，则需要进行后续检验，来得到具体的算法之间的差异。常用的就是Nemenyi后续检验。Nemenyi检验计算出平均序值差别的临界值域，下表是常用的qa值，若两个算法的平均序值差超出了临界值域CD，则相应的置信度1-α拒绝“两个算法性能相同”的假设。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722232932b.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc7222348519.png" alt="12.png"></p>
<p>##<strong>2.7 偏差与方差</strong></p>
<p>偏差-方差分解是解释学习器泛化性能的重要工具。在学习算法中，偏差指的是预测的期望值与真实值的偏差，方差则是每一次预测值与预测值得期望之间的差均方。实际上，偏差体现了学习器预测的准确度，而方差体现了学习器预测的稳定性。通过对泛化误差的进行分解，可以得到：</p>
<ul>
<li><strong>期望泛化误差&#x3D;方差+偏差</strong>	</li>
<li><strong>偏差刻画学习器的拟合能力</strong></li>
<li><strong>方差体现学习器的稳定性</strong></li>
</ul>
<p>易知：方差和偏差具有矛盾性，这就是常说的偏差-方差窘境（bias-variance dilamma），随着训练程度的提升，期望预测值与真实值之间的差异越来越小，即偏差越来越小，但是另一方面，随着训练程度加大，学习算法对数据集的波动越来越敏感，方差值越来越大。换句话说：在欠拟合时，偏差主导泛化误差，而训练到一定程度后，偏差越来越小，方差主导了泛化误差。因此训练也不要贪杯，适度辄止。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722234b09f.png" alt="13.png"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/周志华《Machine Learning》学习笔记(5)--决策树"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(5)--%E5%86%B3%E7%AD%96%E6%A0%91/"
    >西瓜书</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(5)--%E5%86%B3%E7%AD%96%E6%A0%91/" class="article-date">
  <time datetime="2023-06-21T03:43:25.000Z" itemprop="datePublished">2023-06-21</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>上篇主要介绍和讨论了线性模型。首先从最简单的最小二乘法开始，讨论输入属性有一个和多个的情形，接着通过广义线性模型延伸开来，将预测连续值的回归问题转化为分类问题，从而引入了对数几率回归，最后线性判别分析LDA将样本点进行投影，多分类问题实质上通过划分的方法转化为多个二分类问题进行求解。本篇将讨论另一种被广泛使用的分类算法–决策树（Decision Tree）。</p>
<p>#<strong>4、决策树</strong></p>
<p>##<strong>4.1 决策树基本概念</strong></p>
<p>顾名思义，决策树是基于树结构来进行决策的，在网上看到一个例子十分有趣，放在这里正好合适。现想象一位捉急的母亲想要给自己的女娃介绍一个男朋友，于是有了下面的对话：</p>
<hr>
<pre><code>  女儿：多大年纪了？
  母亲：26。
  女儿：长的帅不帅？
  母亲：挺帅的。
  女儿：收入高不？
  母亲：不算很高，中等情况。
  女儿：是公务员不？
  母亲：是，在税务局上班呢。
  女儿：那好，我去见见。
</code></pre>
<hr>
<p>这个女孩的挑剔过程就是一个典型的决策树，即相当于通过年龄、长相、收入和是否公务员将男童鞋分为两个类别：见和不见。假设这个女孩对男人的要求是：30岁以下、长相中等以上并且是高收入者或中等以上收入的公务员，那么使用下图就能很好地表示女孩的决策逻辑（即一颗决策树）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec84a77.png" alt="1.png"></p>
<p>在上图的决策树中，决策过程的每一次判定都是对某一属性的“测试”，决策最终结论则对应最终的判定结果。一般一颗决策树包含：一个根节点、若干个内部节点和若干个叶子节点，易知：</p>
<pre><code>* 每个非叶节点表示一个特征属性测试。
* 每个分支代表这个特征属性在某个值域上的输出。
* 每个叶子节点存放一个类别。
* 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集。
</code></pre>
<p>##<strong>4.2 决策树的构造</strong></p>
<p>决策树的构造是一个递归的过程，有三种情形会导致递归返回：(1) 当前结点包含的样本全属于同一类别，这时直接将该节点标记为叶节点，并设为相应的类别；(2) 当前属性集为空，或是所有样本在所有属性上取值相同，无法划分，这时将该节点标记为叶节点，并将其类别设为该节点所含样本最多的类别；(3) 当前结点包含的样本集合为空，不能划分，这时也将该节点标记为叶节点，并将其类别设为父节点中所含样本最多的类别。算法的基本流程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ecc27fe.png" alt="2.png"></p>
<p>可以看出：决策树学习的关键在于如何选择划分属性，不同的划分属性得出不同的分支结构，从而影响整颗决策树的性能。属性划分的目标是让各个划分出来的子节点尽可能地“纯”，即属于同一类别。因此下面便是介绍量化纯度的具体方法，决策树最常用的算法有三种：ID3，C4.5和CART。</p>
<p>###<strong>4.2.1 ID3算法</strong></p>
<p>ID3算法使用信息增益为准则来选择划分属性，“信息熵”(information entropy)是度量样本结合纯度的常用指标，假定当前样本集合D中第k类样本所占比例为pk，则样本集合D的信息熵定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec515a5.png" alt="3.png"></p>
<p>假定通过属性划分样本集D，产生了V个分支节点，v表示其中第v个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大。故可以计算出划分后相比原始数据集D获得的“信息增益”（information gain）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec3e067.png" alt="4.png"></p>
<p>信息增益越大，表示使用该属性划分样本集D的效果越好，因此ID3算法在递归过程中，每次选择最大信息增益的属性作为当前的划分属性。</p>
<p>###<strong>4.2.2 C4.5算法</strong></p>
<p>ID3算法存在一个问题，就是偏向于取值数目较多的属性，例如：如果存在一个唯一标识，这样样本集D将会被划分为|D|个分支，每个分支只有一个样本，这样划分后的信息熵为零，十分纯净，但是对分类毫无用处。因此C4.5算法使用了“增益率”（gain ratio）来选择划分属性，来避免这个问题带来的困扰。首先使用ID3算法计算出信息增益高于平均水平的候选属性，接着C4.5计算这些候选属性的增益率，增益率定义为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec69647.png" alt="5.png"></p>
<p>###<strong>4.2.3 CART算法</strong></p>
<p>CART决策树使用“基尼指数”（Gini index）来选择划分属性，基尼指数反映的是从样本集D中随机抽取两个样本，其类别标记不一致的概率，因此Gini(D)越小越好，基尼指数定义如下：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec5a2ff.png" alt="6.png"></p>
<p>进而，使用属性α划分后的基尼指数为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec62eaf.png" alt="7.png"></p>
<p>##<strong>4.3 剪枝处理</strong></p>
<p>从决策树的构造流程中我们可以直观地看出：不管怎么样的训练集，决策树总是能很好地将各个类别分离开来，这时就会遇到之前提到过的问题：过拟合（overfitting），即太依赖于训练样本。剪枝（pruning）则是决策树算法对付过拟合的主要手段，剪枝的策略有两种如下：</p>
<pre><code>* 预剪枝（prepruning）：在构造的过程中先评估，再考虑是否分支。
* 后剪枝（post-pruning）：在构造好一颗完整的决策树后，自底向上，评估分支的必要性。
</code></pre>
<p>评估指的是性能度量，即决策树的泛化性能。之前提到：可以使用测试集作为学习器泛化性能的近似，因此可以将数据集划分为训练集和测试集。预剪枝表示在构造数的过程中，对一个节点考虑是否分支时，首先计算决策树不分支时在测试集上的性能，再计算分支之后的性能，若分支对性能没有提升，则选择不分支（即剪枝）。后剪枝则表示在构造好一颗完整的决策树后，从最下面的节点开始，考虑该节点分支对模型的性能是否有提升，若无则剪枝，即将该节点标记为叶子节点，类别标记为其包含样本最多的类别。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec80d34.png" alt="8.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9e330.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc728ec9d497.png" alt="10.png"></p>
<p>上图分别表示不剪枝处理的决策树、预剪枝决策树和后剪枝决策树。预剪枝处理使得决策树的很多分支被剪掉，因此大大降低了训练时间开销，同时降低了过拟合的风险，但另一方面由于剪枝同时剪掉了当前节点后续子节点的分支，因此预剪枝“贪心”的本质阻止了分支的展开，在一定程度上带来了欠拟合的风险。而后剪枝则通常保留了更多的分支，因此采用后剪枝策略的决策树性能往往优于预剪枝，但其自底向上遍历了所有节点，并计算性能，训练时间开销相比预剪枝大大提升。</p>
<p>##<strong>4.4 连续值与缺失值处理</strong></p>
<p>对于连续值的属性，若每个取值作为一个分支则显得不可行，因此需要进行离散化处理，常用的方法为二分法，基本思想为：给定样本集D与连续属性α，二分法试图找到一个划分点t将样本集D在属性α上分为≤t与＞t。</p>
<pre><code>* 首先将α的所有取值按升序排列，所有相邻属性的均值作为候选划分点（n-1个，n为α所有的取值数目）。
* 计算每一个划分点划分集合D（即划分为两个分支）后的信息增益。
* 选择最大信息增益的划分点作为最优划分点。
</code></pre>
<p><img src="https://i.loli.net/2018/10/17/5bc72a0968fad.png" alt="11.png"></p>
<p>现实中常会遇到不完整的样本，即某些属性值缺失。有时若简单采取剔除，则会造成大量的信息浪费，因此在属性值缺失的情况下需要解决两个问题：（1）如何选择划分属性。（2）给定划分属性，若某样本在该属性上缺失值，如何划分到具体的分支上。假定为样本集中的每一个样本都赋予一个权重，根节点中的权重初始化为1，则定义：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a098f3be.png" alt="12.png"></p>
<p>对于（1）：通过在样本集D中选取在属性α上没有缺失值的样本子集，计算在该样本子集上的信息增益，最终的信息增益等于该样本子集划分后信息增益乘以样本子集占样本集的比重。即：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a096ccc3.png" alt="13.png"></p>
<p>对于（2）：若该样本子集在属性α上的值缺失，则将该样本以不同的权重（即每个分支所含样本比例）划入到所有分支节点中。该样本在分支节点中的权重变为：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72a093ed3c.png" alt="14.png"></p>
<p>​    </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MachineLearning/Machine-learning-learning-notes-master/周志华《Machine Learning》学习笔记(4)--线性模型"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(4)--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/"
    >西瓜书</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/06/21/MachineLearning/Machine-learning-learning-notes-master/%E5%91%A8%E5%BF%97%E5%8D%8E%E3%80%8AMachine%20Learning%E3%80%8B%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0(4)--%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/" class="article-date">
  <time datetime="2023-06-21T03:43:25.000Z" itemprop="datePublished">2023-06-21</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>笔记的前一部分主要是对机器学习预备知识的概括，包括机器学习的定义&#x2F;术语、学习器性能的评估&#x2F;度量以及比较，本篇之后将主要对具体的学习算法进行理解总结，本篇则主要是第3章的内容–线性模型。</p>
<p>#<strong>3、线性模型</strong></p>
<p>谈及线性模型，其实我们很早就已经与它打过交道，还记得高中数学必修3课本中那个顽皮的“最小二乘法”吗？这就是线性模型的经典算法之一：根据给定的（x，y）点对，求出一条与这些点拟合效果最好的直线y&#x3D;ax+b，之前我们利用下面的公式便可以计算出拟合直线的系数a,b（3.1中给出了具体的计算过程），从而对于一个新的x，可以预测它所对应的y值。前面我们提到：在机器学习的术语中，当预测值为连续值时，称为“回归问题”，离散值时为“分类问题”。本篇先从线性回归任务开始，接着讨论分类和多分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b068e48.png" alt="1.png"></p>
<p>##<strong>3.1 线性回归</strong></p>
<p>线性回归问题就是试图学到一个线性模型尽可能准确地预测新样本的输出值，例如：通过历年的人口数据预测2017年人口数量。在这类问题中，往往我们会先得到一系列的有标记数据，例如：2000–&gt;13亿…2016–&gt;15亿，这时输入的属性只有一个，即年份；也有输入多属性的情形，假设我们预测一个人的收入，这时输入的属性值就不止一个了，例如：（学历，年龄，性别，颜值，身高，体重）–&gt;15k。</p>
<p>有时这些输入的属性值并不能直接被我们的学习模型所用，需要进行相应的处理，对于连续值的属性，一般都可以被学习器所用，有时会根据具体的情形作相应的预处理，例如：归一化等；对于离散值的属性，可作下面的处理：</p>
<ul>
<li><p>若属性值之间存在“序关系”，则可以将其转化为连续值，例如：身高属性分为“高”“中等”“矮”，可转化为数值：{1， 0.5， 0}。</p>
</li>
<li><p>若属性值之间不存在“序关系”，则通常将其转化为向量的形式，例如：性别属性分为“男”“女”，可转化为二维向量：{（1，0），（0，1）}。</p>
</li>
</ul>
<p>（1）当输入属性只有一个的时候，就是最简单的情形，也就是我们高中时最熟悉的“最小二乘法”（Euclidean distance），首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线y&#x3D;wx+b的两个参数w和b，计算过程如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ccec4.png" alt="2.png"></p>
<p>（2）当输入属性有多个的时候，例如对于一个样本有d个属性{（x1,x2…xd）,y}，则y&#x3D;wx+b需要写成：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc72567b8bcd.png" alt="0.png"></p>
<p>通常对于多元问题，常常使用矩阵的形式来表示数据。在本问题中，将具有m个样本的数据集表示成矩阵X，将系数w与b合并成一个列向量，这样每个样本的预测值以及所有样本的均方误差最小化就可以写成下面的形式：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0ad8f7.png" alt="3.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0af652.png" alt="4.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b090543.png" alt="5.png"></p>
<p>同样地，我们使用最小二乘法对w和b进行估计，令均方误差的求导等于0，需要注意的是，当一个矩阵的行列式不等于0时，我们才可能对其求逆，因此对于下式，我们需要考虑矩阵（X的转置*X）的行列式是否为0，若不为0，则可以求出其解，若为0，则需要使用其它的方法进行计算，书中提到了引入正则化，此处不进行深入。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0cde33.png" alt="6.png"></p>
<p>另一方面，有时像上面这种原始的线性回归可能并不能满足需求，例如：y值并不是线性变化，而是在指数尺度上变化。这时我们可以采用线性模型来逼近y的衍生物，例如lny，这时衍生的线性模型如下所示，实际上就是相当于将指数曲线投影在一条直线上，如下图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b103cbf.png" alt="7.png"></p>
<p>更一般地，考虑所有y的衍生物的情形，就得到了“广义的线性模型”（generalized linear model），其中，g（*）称为联系函数（link function）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a2841.png" alt="8.png"></p>
<p>##<strong>3.2 线性几率回归</strong></p>
<p>回归就是通过输入的属性值得到一个预测值，利用上述广义线性模型的特征，是否可以通过一个联系函数，将预测值转化为离散值从而进行分类呢？线性几率回归正是研究这样的问题。对数几率引入了一个对数几率函数（logistic function）,将预测值投影到0-1之间，从而将线性回归问题转化为二分类问题。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0c7748.png" alt="9.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc722b0a655d.png" alt="10.png"></p>
<p>若将y看做样本为正例的概率，（1-y）看做样本为反例的概率，则上式实际上使用线性回归模型的预测结果器逼近真实标记的对数几率。因此这个模型称为“对数几率回归”（logistic regression），也有一些书籍称之为“逻辑回归”。下面使用最大似然估计的方法来计算出w和b两个参数的取值，下面只列出求解的思路，不列出具体的计算过程。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b824f0c.png" alt="11.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b817961.png" alt="12.png"></p>
<p>##<strong>3.3 线性判别分析</strong></p>
<p>线性判别分析（Linear Discriminant Analysis，简称LDA）,其基本思想是：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。如图所示：</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b863ebb.png" alt="13.png"><img src="https://i.loli.net/2018/10/17/5bc723b85bfa9.png" alt="14.png"></p>
<p>想让同类样本点的投影点尽可能接近，不同类样本点投影之间尽可能远，即：让各类的协方差之和尽可能小，不用类之间中心的距离尽可能大。基于这样的考虑，LDA定义了两个散度矩阵。</p>
<ul>
<li>类内散度矩阵（within-class scatter matrix）</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8156e1.png" alt="15.png"></p>
<ul>
<li>类间散度矩阵(between-class scaltter matrix)</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e9db3.png" alt="16.png"></p>
<p>因此得到了LDA的最大化目标：“广义瑞利商”（generalized Rayleigh quotient）。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b7e8a61.png" alt="17.png"></p>
<p>从而分类问题转化为最优化求解w的问题，当求解出w后，对新的样本进行分类时，只需将该样本点投影到这条直线上，根据与各个类别的中心值进行比较，从而判定出新样本与哪个类别距离最近。求解w的方法如下所示，使用的方法为λ乘子。</p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b83d5e0.png" alt="18.png"></p>
<p>若将w看做一个投影矩阵，类似PCA的思想，则LDA可将样本投影到N-1维空间（N为类簇数），投影的过程使用了类别信息（标记信息），因此LDA也常被视为一种经典的监督降维技术。<br>​<br>##<strong>3.4 多分类学习</strong></p>
<p>现实中我们经常遇到不只两个类别的分类问题，即多分类问题，在这种情形下，我们常常运用“拆分”的策略，通过多个二分类学习器来解决多分类问题，即将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。最为经典的拆分策略有三种：“一对一”（OvO）、“一对其余”（OvR）和“多对多”（MvM），核心思想与示意图如下所示。</p>
<ul>
<li><p>OvO：给定数据集D，假定其中有N个真实类别，将这N个类别进行两两配对（一个正类&#x2F;一个反类），从而产生N（N-1）&#x2F;2个二分类学习器，在测试阶段，将新样本放入所有的二分类学习器中测试，得出N（N-1）个结果，最终通过投票产生最终的分类结果。</p>
</li>
<li><p>OvM：给定数据集D，假定其中有N个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生N个二分类学习器，在测试阶段，得出N个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。</p>
</li>
<li><p>MvM：给定数据集D，假定其中有N个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了M次划分，则生成了M个二分类学习器，在测试阶段（解码），得出M个结果组成一个新的码，最终通过计算海明&#x2F;欧式距离选择距离最小的类别作为最终分类结果。</p>
</li>
</ul>
<p><img src="https://i.loli.net/2018/10/17/5bc723b862bfb.png" alt="19.png"></p>
<p><img src="https://i.loli.net/2018/10/17/5bc723b8300d5.png" alt="20.png"></p>
<p>##<strong>3.5 类别不平衡问题</strong></p>
<p>类别不平衡（class-imbanlance）就是指分类问题中不同类别的训练样本相差悬殊的情况，例如正例有900个，而反例只有100个，这个时候我们就需要进行相应的处理来平衡这个问题。常见的做法有三种：</p>
<ol>
<li>在训练样本较多的类别中进行“欠采样”（undersampling）,比如从正例中采出100个，常见的算法有：EasyEnsemble。</li>
<li>在训练样本较少的类别中进行“过采样”（oversampling）,例如通过对反例中的数据进行插值，来产生额外的反例，常见的算法有SMOTE。</li>
<li>直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中再缩放也是代价敏感学习的基础。<img src="https://i.loli.net/2018/10/17/5bc726fe87ae2.png" alt="21.png"></li>
</ol>
<p>​<br>​      </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/ML/" rel="tag">ML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-giligili"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
     
    <div class="article-meta">
      <a href="/2023/05/01/giligili/" class="article-date">
  <time datetime="2023-05-01T08:02:09.851Z" itemprop="datePublished">2023-05-01</time>
</a>    
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p><img src="/./markdown-img/giligili.assets/image-20230501160218413.png" alt="image-20230501160218413"></p>
<p>为什么直接使用display flex就能直接让flex的内容横过来</p>
<p>注释写好，是为了方便查找，ctrl f头部就能找到头部的所有样式</p>
<p><img src="/./markdown-img/giligili.assets/image-20230501162247727.png" alt="image-20230501162247727"></p>
<p><img src="/./markdown-img/giligili.assets/image-20230501162235824.png" alt="image-20230501162235824"></p>
<p>在f12计算样式里查样式</p>
<p>去掉边框线</p>
<p><img src="/./markdown-img/giligili.assets/image-20230501170906132.png" alt="image-20230501170906132"></p>
<p>padding ：上下px  左右px；</p>
<p>win+v剪切板</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-前端学习笔记/其它/Q0-intrest"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/05/01/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/Q0-intrest/"
    >静态页面-1girigiri_love</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/05/01/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/Q0-intrest/" class="article-date">
  <time datetime="2023-05-01T03:34:47.000Z" itemprop="datePublished">2023-05-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/code/">code</a> / <a class="article-category-link" href="/categories/code/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h1 id="1-视差滚动"><a href="#1-视差滚动" class="headerlink" title="1.视差滚动"></a>1.视差滚动</h1><p>可以查范例看看</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CSS/" rel="tag">CSS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HTML/" rel="tag">HTML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-前端学习笔记/静态页面练习/girigiri_love"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2023/05/01/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%BB%83%E4%B9%A0/girigiri_love/"
    >静态页面-1girigiri_love</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2023/05/01/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E9%9D%99%E6%80%81%E9%A1%B5%E9%9D%A2%E7%BB%83%E4%B9%A0/girigiri_love/" class="article-date">
  <time datetime="2023-05-01T03:34:47.000Z" itemprop="datePublished">2023-05-01</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/code/">code</a> / <a class="article-category-link" href="/categories/code/note/">note</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <h2 id="1-一个问题"><a href="#1-一个问题" class="headerlink" title="1.一个问题"></a>1.一个问题</h2><p>想实现的效果：head里的‘个人’图标，想要实现红色圆形底色，且‘个人’图标需要跟其它图标大小相等。且内容部分不会由于宽度的原因冒出头来。</p>
<ol>
<li><h3 id="用broder-box"><a href="#用broder-box" class="headerlink" title="用broder-box"></a>用broder-box</h3><p>直接用会导致‘个人’图标缩小</p>
<img src="./../_posts/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/markdown-img/girigiri_love.assets/image-20230505225632986.png" alt="image-20230505225632986" style="zoom:50%;" />
</li>
<li><h3 id="不用broder-box"><a href="#不用broder-box" class="headerlink" title="不用broder-box"></a>不用broder-box</h3><p>主体部分中，很多部分都需要设定宽度100%，又需要设定position：absolute</p>
<img src="./../_posts/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/markdown-img/girigiri_love.assets/image-20230505225731776.png" alt="image-20230505225731776" style="zoom:33%;" />
</li>
<li><h3 id="总结：用broder-box，所以要改‘个人部分’，最好的办法是"><a href="#总结：用broder-box，所以要改‘个人部分’，最好的办法是" class="headerlink" title="总结：用broder-box，所以要改‘个人部分’，最好的办法是"></a>总结：用broder-box，所以要改‘个人部分’，最好的办法是</h3><ol>
<li><p>利用选择器的优先级原理，给‘个人’的·上一层加一个id&#x3D;user。上一层本身的class选择器不用管。</p>
<p>重复的部分会被id选择器覆盖。</p>
<p><img src="./../_posts/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/markdown-img/girigiri_love.assets/image-20230505232718506.png" alt="image-20230505232718506" style="zoom:33%;" /><img src="./../_posts/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/markdown-img/girigiri_love.assets/image-20230505233056969.png" alt="image-20230505233056969" style="zoom: 50%;" /></p>
</li>
<li><p>设置它的宽高使其（装‘个人’图片的盒子）成为一个正方形，然后设置border-radius。&#x2F;&#x2F;底部红圆</p>
</li>
<li><p>然后通过设置padding来合理缩小图片。</p>
<img src="./../_posts/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/markdown-img/girigiri_love.assets/1683300580219.png" alt="1683300580219" style="zoom:38%;" /></li>
</ol>
</li>
</ol>
<h2 id="2-一个知识点"><a href="#2-一个知识点" class="headerlink" title="2.一个知识点"></a>2.一个知识点</h2><p>position：absolute是在偏移，它如果设了left 20等，这和width：100%是互斥的。所以要用broder-box；</p>
<h2 id="3-一个技巧"><a href="#3-一个技巧" class="headerlink" title="3.一个技巧"></a>3.一个技巧</h2><p>debug可以在浏览器端*{border: 1px solid green;}</p>
<h2 id="4-最终效果-底部红字是桌面歌词"><a href="#4-最终效果-底部红字是桌面歌词" class="headerlink" title="4.最终效果(底部红字是桌面歌词)"></a>4.最终效果(底部红字是桌面歌词)</h2><p><img src="/./../_posts/%E5%89%8D%E7%AB%AF%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/%E5%85%B6%E5%AE%83/markdown-img/girigiri_love.assets/image-20230505234450766.png" alt="image-20230505234450766"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/CSS/" rel="tag">CSS</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/HTML/" rel="tag">HTML</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/">上一页</a><a class="page-number" href="/">1</a><span class="page-number current">2</span><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><span class="space">&hellip;</span><a class="page-number" href="/page/8/">8</a><a class="extend next" rel="next" href="/page/3/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2023
        <i class="ri-heart-fill heart_icon"></i> LIXiaohan
      </li>
    </ul>
    <ul>
      <li>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
      <li>
          <img src="/images/beian.png"></img>
          <a href="http://www.beian.gov.cn/portal/registerSystemInfo?recordcode=01234567890123" target="_black" rel="nofollow">黑公网安备0123456789号</a>
      </li>
        
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src=''></script>
        
      </li>
    </ul>
  </div>
</footer>    
    </main>
    <div class="float_btns">
      <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

    </div>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/images/ayer-side.svg" alt="Pravite Home"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="/images/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="/images/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-3.6.0.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.staticfile.org/jquery-modal/0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.staticfile.org/justifiedGallery/3.8.1/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.staticfile.org/photoswipe/4.1.3/default-skin/default-skin.min.css">
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe.min.js"></script>
<script src="https://cdn.staticfile.org/photoswipe/4.1.3/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->
 <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
      tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
  });

  MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for(i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
      }
  });
</script>

<script src="https://cdn.staticfile.org/mathjax/2.7.7/MathJax.js"></script>
<script src="https://cdn.staticfile.org/mathjax/2.7.7/config/TeX-AMS-MML_HTMLorMML-full.js"></script>
<script>
  var ayerConfig = {
    mathjax: true,
  };
</script>

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->
 
<script src="/js/clickBoom2.js"></script>
 
<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.staticfile.org/clipboard.js/2.0.10/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
    <div id="music">
    
    
    
    <iframe frameborder="no" border="1" marginwidth="0" marginheight="0" width="200" height="52"
        src="//music.163.com/outchain/player?type=2&id=22707008&auto=1&height=32"></iframe>
</div>

<style>
    #music {
        position: fixed;
        right: 15px;
        bottom: 0;
        z-index: 998;
    }
</style>
    
    
<script>
  const password = "11";
  const lock_password = window.sessionStorage.getItem("lock_password");
  console.log(password, lock_password);
  if (lock_password !== password) {
    Swal.fire({
      title: "请输入访问密码",
      input: "text",
      inputAttributes: {
        autocapitalize: "off",
      },
      showCancelButton: false,
      showLoaderOnConfirm: true,
      allowOutsideClick: false,
      confirmButtonText: "确定",
    }).then((result) => {
      console.log(result);
      if (result.isConfirmed) {
        console.log(password);
        if (result.value === password) {
          window.sessionStorage.setItem("lock_password", result.value);
        } else {
          Swal.fire({
            icon: "error",
            title: "密码错误，请重试",
            confirmButtonText: "确定",
            allowOutsideClick: false,
          }).then(() => {
            window.location.reload();
          });
        }
      }
    });
  }
</script>


  </div>
</body>

</html>